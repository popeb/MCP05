Guideline to the deep autoencode analysis
=============================================
To run the following analysis you need to have installed the dimer library (from https://bitbucket.org/gertidenas/dimer). 

The analysis has the following steps:

Generate the dataset from peaks ::

   ~$ make 

this will create an archive named *W500000_B200.h5* that contains
two datasets: train200 and full. The former, contains 200 random
input examples and is used to train the dAE stacks. The latter
contains the full data ::

   ~$ h5ls W500000_B200.h5 
   full                     Group
   train200                 Group
   ~$ 

At this point we are ready to train the dAE. ::

  ~$ cd dAE
  ~$ make

this will train 10 stacks of dAEs and put the results
on the *W500000_B200.h5* archive ::

   ~$ h5ls W500000_B200.h5/train200
   T                        Group
   X                        Group
   Y                        Group
   meanX                    Group
   rawX                     Group
   sdX                      Group
   tr__20140521_120030_932473_ Group
   tr__20140522_202446_624138_ Group
   tr__20140523_054555_945980_ Group
   tr__20140523_150248_748763_ Group
   tr__20140524_004625_757278_ Group
   tr__20140524_103805_323890_ Group
   tr__20140524_194954_317683_ Group
   tr__20140525_025131_166277_ Group
   tr__20140525_123336_212649_ Group
   tr__20140525_214329_203744_ Group
   tr__20140526_063844_621310_ Group
   ~$

now we can compute the representations of the full input generated by
the trained dAE stacks at each layer. ::

   ~$ pwd
   dAE
   ~$ dump_representations.sh ../W500000_B200.h5
   ...
   ~$ ls 
   tr__20140521_120030_932473_.0.repr  tr__20140523_054555_945980_.1.repr  tr__20140524_004625_757278_.2.repr  tr__20140524_194954_317683_.3.repr  tr__20140525_214329_203744_.0.repr
   tr__20140521_120030_932473_.1.repr  tr__20140523_054555_945980_.2.repr  tr__20140524_004625_757278_.3.repr  tr__20140525_025131_166277_.0.repr  tr__20140525_214329_203744_.1.repr
   tr__20140521_120030_932473_.2.repr  tr__20140523_054555_945980_.3.repr  tr__20140524_103805_323890_.0.repr  tr__20140525_025131_166277_.1.repr  tr__20140525_214329_203744_.2.repr
   tr__20140521_120030_932473_.3.repr  tr__20140523_150248_748763_.0.repr  tr__20140524_103805_323890_.1.repr  tr__20140525_025131_166277_.2.repr  tr__20140525_214329_203744_.3.repr
   tr__20140522_202446_624138_.0.repr  tr__20140523_150248_748763_.1.repr  tr__20140524_103805_323890_.2.repr  tr__20140525_025131_166277_.3.repr  tr__20140526_063844_621310_.0.repr
   tr__20140522_202446_624138_.1.repr  tr__20140523_150248_748763_.2.repr  tr__20140524_103805_323890_.3.repr  tr__20140525_123336_212649_.0.repr  tr__20140526_063844_621310_.1.repr
   tr__20140522_202446_624138_.2.repr  tr__20140523_150248_748763_.3.repr  tr__20140524_194954_317683_.0.repr  tr__20140525_123336_212649_.1.repr  tr__20140526_063844_621310_.2.repr
   tr__20140522_202446_624138_.3.repr  tr__20140524_004625_757278_.0.repr  tr__20140524_194954_317683_.1.repr  tr__20140525_123336_212649_.2.repr  tr__20140526_063844_621310_.3.repr
   tr__20140523_054555_945980_.0.repr  tr__20140524_004625_757278_.1.repr  tr__20140524_194954_317683_.2.repr  tr__20140525_123336_212649_.3.repr


for each trained model there are 4 files, one / layer. Next can build the 
representations from PCA by running the ./compute_pca.py script from the 
PCA directory.

This concludes the data generation. Now run the *a.py* script to create temporary files.
Finally, run the model_comparison.ipynb IPython notebook to generate the figures.
